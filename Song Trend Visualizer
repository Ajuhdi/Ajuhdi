import sys
import time
from datetime import datetime

import matplotlib
matplotlib.use("TkAgg")              # ← add this line
import matplotlib.pyplot as plt       # ← keep this import after the line above

import requests
from requests.adapters import HTTPAdapter, Retry
from bs4 import BeautifulSoup
import pandas as pd
from fuzzywuzzy import fuzz


# ---------- Networking helpers ----------
def make_session():
    s = requests.Session()
    # Polite UA + retries for transient errors
    s.headers.update({
        "User-Agent": "SongTrendBot/1.0 (+https://example.com) Python requests"
    })
    retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])
    s.mount("https://", HTTPAdapter(max_retries=retries))
    s.mount("http://", HTTPAdapter(max_retries=retries))
    return s


SESSION = make_session()


# ---------- Scraping ----------
def get_archive_links():
    url = "https://kworb.net/ww/archive/"
    resp = SESSION.get(url, timeout=15)
    resp.raise_for_status()

    soup = BeautifulSoup(resp.content, "html.parser")
    links = []
    for a in soup.find_all("a"):
        t = a.text.strip()
        if t.endswith(".html") and t[:-5].isdigit():
            links.append(t.replace(".html", ""))  # YYYYMMDD
    return links


def scrape_data_for_song(archive_links, start_date, end_date, song_name, min_score=80):
    # Ensure chronological range and filter archive pages up front
    if start_date > end_date:
        start_date, end_date = end_date, start_date

    pages = [d for d in archive_links if start_date <= d <= end_date]
    pages.sort()

    scraped_data = []
    for date in pages:
        url = f"https://kworb.net/ww/archive/{date}.html"
        try:
            resp = SESSION.get(url, timeout=20)
            if not resp.ok:
                continue
        except requests.RequestException:
            continue

        soup = BeautifulSoup(resp.content.decode("utf-8", errors="ignore"), "html.parser")
        table = soup.find("table")
        if not table:
            continue

        rows = table.find_all("tr")[1:]  # skip header
        for row in rows:
            cols = row.find_all("td")
            if len(cols) > 2:
                position = cols[0].get_text(strip=True)
                song_title = cols[2].get_text(strip=True)

                # Fuzzy match on title
                match_score = fuzz.partial_ratio(song_name.lower().strip(), song_title.lower().strip())
                if match_score >= min_score:
                    scraped_data.append({
                        "Date": date,
                        "Position": position,
                        "Match Score": match_score,
                        "Song Title": song_title
                    })
        # be polite
        time.sleep(0.2)

    return scraped_data


# ---------- Data wrangling & viz ----------
def save_to_dataframe(data):
    df = pd.DataFrame(data)
    if df.empty:
        raise ValueError("No data was scraped (no title matches in the selected date range).")
    df["Date"] = pd.to_datetime(df["Date"], format="%Y%m%d")
    df["Position"] = pd.to_numeric(df["Position"], errors="coerce")
    df = df.dropna(subset=["Position"]).sort_values("Date")
    return df


def visualize_song_trend(song_name, df):
    if df.empty:
        print("No data to visualize.")
        return

    df = df.sort_values("Date").copy()

    # Rolling average (7-day window over daily archives)
    df["Position_Smoothed"] = df["Position"].rolling(window=7, center=True, min_periods=1).mean()

    # Plot raw vs smoothed
    plt.figure(figsize=(12, 6))
    plt.plot(df["Date"], df["Position"], alpha=0.35, label=f"{song_name} (Raw)")
    plt.plot(df["Date"], df["Position_Smoothed"], marker="o", label=f"{song_name} (7-day avg)")
    plt.gca().invert_yaxis()
    plt.title(f"Trend for '{song_name}' Over Time")
    plt.xlabel("Date")
    plt.ylabel("Chart Position (Lower is Better)")
    plt.legend()
    plt.tight_layout()
    plt.show()

    # Monthly average
    df["Month"] = df["Date"].dt.to_period("M")
    monthly_avg = df.groupby("Month", as_index=False)["Position"].mean()
    monthly_avg["MonthTS"] = monthly_avg["Month"].dt.to_timestamp()

    plt.figure(figsize=(12, 6))
    plt.plot(monthly_avg["MonthTS"], monthly_avg["Position"], marker="o", label=f"{song_name} (Monthly Avg)")
    plt.gca().invert_yaxis()
    plt.title(f"Monthly Trend for '{song_name}'")
    plt.xlabel("Month")
    plt.ylabel("Average Chart Position (Lower is Better)")
    plt.legend()
    plt.tight_layout()
    plt.show()


# ---------- CLI ----------
def read_yyyymmdd(prompt):
    s = input(prompt).strip()
    try:
        # validate but keep as string for archive comparison
        datetime.strptime(s, "%Y%m%d")
        return s
    except ValueError:
        print("Please use YYYYMMDD (e.g., 20231125).")
        sys.exit(1)


def main():
    print("Welcome to the Song Trend Visualization Program!")

    try:
        archive_links = get_archive_links()
    except requests.RequestException as e:
        print(f"Failed to fetch archive index: {e}")
        sys.exit(1)

    start_date = read_yyyymmdd("Enter the first date (YYYYMMDD): ")
    end_date   = read_yyyymmdd("Enter the second date (YYYYMMDD): ")
    song_name  = input("Enter the name of the song: ").strip()

    historical_data = scrape_data_for_song(archive_links, start_date, end_date, song_name)

    try:
        df = save_to_dataframe(historical_data)
    except ValueError as e:
        print(f"Error: {e}")
        sys.exit(0)

    visualize_song_trend(song_name, df)


if __name__ == "__main__":
    main()
